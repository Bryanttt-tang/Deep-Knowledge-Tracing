
Dataset: assist2009, Learning Rate: 0.002

loading train data:    : 3361it [00:07, 472.94it/s]
loading test data:    : 856it [00:00, 2150.75it/s]
epoch: 0
Training:    :   0%|          | 0/160 [00:00<?, ?it/s]..\evaluation\eval.py:31: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  a = (((batch[student][:, 0:C.NUM_OF_QUESTIONS] - batch[student][:, C.NUM_OF_QUESTIONS:]).sum(1) + 1)//2)[1:]
> d:\ethz\internship\adaptive-e-learning-for-educational-recommendation-system\deepknowledgetracing-dkt-pytorch\dkt\knowledgetracing\evaluation\eval.py(35)forward()
-> loss = loss - (a[i]*torch.log(p[i]) + (1-a[i])*torch.log(1-p[i]))
(Pdb) > d:\ethz\internship\adaptive-e-learning-for-educational-recommendation-system\deepknowledgetracing-dkt-pytorch\dkt\knowledgetracing\evaluation\eval.py(34)forward()
-> # breakpoint()
(Pdb) > d:\ethz\internship\adaptive-e-learning-for-educational-recommendation-system\deepknowledgetracing-dkt-pytorch\dkt\knowledgetracing\evaluation\eval.py(35)forward()
-> loss = loss - (a[i]*torch.log(p[i]) + (1-a[i])*torch.log(1-p[i]))
(Pdb) > d:\ethz\internship\adaptive-e-learning-for-educational-recommendation-system\deepknowledgetracing-dkt-pytorch\dkt\knowledgetracing\evaluation\eval.py(34)forward()
-> # breakpoint()
(Pdb) > d:\ethz\internship\adaptive-e-learning-for-educational-recommendation-system\deepknowledgetracing-dkt-pytorch\dkt\knowledgetracing\evaluation\eval.py(35)forward()
-> loss = loss - (a[i]*torch.log(p[i]) + (1-a[i])*torch.log(1-p[i]))
(Pdb) > d:\ethz\internship\adaptive-e-learning-for-educational-recommendation-system\deepknowledgetracing-dkt-pytorch\dkt\knowledgetracing\evaluation\eval.py(34)forward()
-> # breakpoint()
(Pdb) > d:\ethz\internship\adaptive-e-learning-for-educational-recommendation-system\deepknowledgetracing-dkt-pytorch\dkt\knowledgetracing\evaluation\eval.py(35)forward()
-> loss = loss - (a[i]*torch.log(p[i]) + (1-a[i])*torch.log(1-p[i]))
(Pdb)
Training:    :   0%|          | 0/160 [00:11<?, ?it/s]
Traceback (most recent call last):
  File "D:\ETHz\Internship\adaptive-e-learning-for-educational-recommendation-system\DeepKnowledgeTracing-DKT-Pytorch\DKT\KnowledgeTracing\evaluation\run.py", line 41, in <module>
    model, optimizer, train_loss = eval.train(trainLoaders, model, optimizer_adgd, loss_func,device)
  File "..\evaluation\eval.py", line 89, in train
    model, optimizer, loss = train_epoch(model, trainLoaders[i], optimizer, lossFunc,device)
  File "..\evaluation\eval.py", line 46, in train_epoch
    loss = loss_func(pred, batch)
  File "C:\Users\tyu06\anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "..\evaluation\eval.py", line 35, in forward
    loss = loss - (a[i]*torch.log(p[i]) + (1-a[i])*torch.log(1-p[i]))
  File "..\evaluation\eval.py", line 35, in forward
    loss = loss - (a[i]*torch.log(p[i]) + (1-a[i])*torch.log(1-p[i]))
  File "C:\Users\tyu06\anaconda3\lib\bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "C:\Users\tyu06\anaconda3\lib\bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit