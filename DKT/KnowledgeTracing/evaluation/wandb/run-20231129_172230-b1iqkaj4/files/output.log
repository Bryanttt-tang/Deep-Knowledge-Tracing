
Dataset: LON_course_comb_df1, Learning Rate: 0.002

loading train data:    : 1666it [00:04, 379.14it/s]
loading test data:    : 412it [00:00, 1080.02it/s]
Training:    :   0%|          | 0/365 [00:00<?, ?it/s]






















































































































































































Training:    : 100%|██████████| 365/365 [07:19<00:00,  1.20s/it]







































































































Testing:    :  80%|████████  | 293/365 [04:53<01:12,  1.00s/it]
Traceback (most recent call last):
  File "/cluster/home/yutang/Deep-Knowledge-Tracing/DKT/KnowledgeTracing/evaluation/./run.py", line 48, in <module>
    train_auc, train_f1, train_recall, train_precision,val_loss=eval.test(trainLoaders, model,loss_func, device)
  File "/cluster/home/yutang/Deep-Knowledge-Tracing/DKT/KnowledgeTracing/evaluation/eval.py", line 108, in test
    for i in range(len(testLoaders)):
  File "/cluster/home/yutang/Deep-Knowledge-Tracing/DKT/KnowledgeTracing/evaluation/eval.py", line 71, in test_epoch
    batch = batch.to(device)
  File "/cluster/home/yutang/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/cluster/home/yutang/Deep-Knowledge-Tracing/DKT/KnowledgeTracing/model/RNNModel.py", line 22, in forward
    out,(hn, cn) = self.rnn(x, (h0, c0))
  File "/cluster/home/yutang/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/cluster/home/yutang/miniconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py", line 812, in forward
    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 23.69 GiB total capacity; 20.23 GiB already allocated; 14.06 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF