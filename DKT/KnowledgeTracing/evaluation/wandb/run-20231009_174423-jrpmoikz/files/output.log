
Dataset: assist2009, Learning Rate: 0.002
loading train data:    : 2020it [00:02, 1009.65it/s]
loading train data:    : 3361it [00:07, 463.58it/s]
loading test data:    : 856it [00:00, 2234.96it/s]
Training:    :   0%|          | 0/160 [00:00<?, ?it/s]..\evaluation\eval.py:31: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  a = (((batch[student][:, 0:C.NUM_OF_QUESTIONS] - batch[student][:, C.NUM_OF_QUESTIONS:]).sum(1) + 1)//2)[1:]
Training:    :   0%|          | 0/160 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "D:\ETHz\Internship\adaptive-e-learning-for-educational-recommendation-system\DeepKnowledgeTracing-DKT-Pytorch\DKT\KnowledgeTracing\evaluation\run.py", line 41, in <module>
    model, optimizer, train_loss = eval.train(trainLoaders, model, optimizer_adgd, loss_func,device)
  File "..\evaluation\eval.py", line 89, in train
    model, optimizer, loss = train_epoch(model, trainLoaders[i], optimizer, lossFunc,device)
  File "..\evaluation\eval.py", line 49, in train_epoch
    optimizer.step()
  File "C:\Users\tyu06\anaconda3\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\tyu06\anaconda3\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\tyu06\anaconda3\lib\site-packages\torch\optim\adagrad.py", line 106, in step
    F.adagrad(params_with_grad,
  File "C:\Users\tyu06\anaconda3\lib\site-packages\torch\optim\_functional.py", line 54, in adagrad
    state_sum.addcmul_(grad, grad, value=1)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!