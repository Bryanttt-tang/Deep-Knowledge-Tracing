
loading train data:    : 0it [00:00, ?it/s]

loading train data:    : 3361it [00:08, 418.67it/s]
loading test data:    : 856it [00:00, 1653.58it/s]
epoch: 0
Training:    :   0%|          | 0/160 [00:00<?, ?it/s]..\evaluation\eval.py:31: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  a = (((batch[student][:, 0:C.NUM_OF_QUESTIONS] - batch[student][:, C.NUM_OF_QUESTIONS:]).sum(1) + 1)//2)[1:]
> d:\ethz\internship\adaptive-e-learning-for-educational-recommendation-system\deepknowledgetracing-dkt-pytorch\dkt\knowledgetracing\evaluation\eval.py(48)train_epoch()
-> optimizer.zero_grad()
(Pdb) tensor([1746.1799], device='cuda:0', grad_fn=<SubBackward0>)
(Pdb) tensor([[[0.5019, 0.4965, 0.5166,  ..., 0.4767, 0.5000, 0.4851],
         [0.5048, 0.5054, 0.5104,  ..., 0.4799, 0.4922, 0.4947],
         [0.5032, 0.4996, 0.5143,  ..., 0.4786, 0.4867, 0.4904],
         ...,
         [0.5246, 0.5132, 0.5056,  ..., 0.4808, 0.4968, 0.4953],
         [0.5246, 0.5132, 0.5056,  ..., 0.4808, 0.4968, 0.4953],
         [0.5246, 0.5132, 0.5056,  ..., 0.4808, 0.4968, 0.4953]],
        [[0.5106, 0.4990, 0.5145,  ..., 0.4793, 0.5142, 0.4823],
         [0.5164, 0.5113, 0.5078,  ..., 0.4789, 0.5093, 0.4905],
         [0.5148, 0.5034, 0.5130,  ..., 0.4772, 0.5003, 0.4875],
         ...,
         [0.5175, 0.5143, 0.4993,  ..., 0.4923, 0.5035, 0.4944],
         [0.5175, 0.5143, 0.4993,  ..., 0.4923, 0.5035, 0.4944],
         [0.5175, 0.5143, 0.4993,  ..., 0.4923, 0.5035, 0.4944]],
        [[0.5029, 0.4979, 0.5221,  ..., 0.4799, 0.4962, 0.4842],
         [0.5125, 0.5016, 0.5183,  ..., 0.4831, 0.4895, 0.4885],
         [0.5131, 0.5021, 0.5112,  ..., 0.4857, 0.4985, 0.4898],
         ...,
         [0.5103, 0.5026, 0.4971,  ..., 0.4866, 0.4967, 0.4984],
         [0.5352, 0.5043, 0.5111,  ..., 0.4819, 0.5019, 0.4878],
         [0.5308, 0.4956, 0.5008,  ..., 0.4901, 0.5049, 0.4830]],
        ...,
        [[0.5086, 0.4881, 0.5184,  ..., 0.4689, 0.5016, 0.4938],
         [0.5199, 0.4977, 0.5133,  ..., 0.4712, 0.5005, 0.5055],
         [0.5197, 0.4888, 0.5119,  ..., 0.4695, 0.4964, 0.5017],
         ...,
         [0.5205, 0.5086, 0.5142,  ..., 0.4814, 0.4960, 0.4837],
         [0.5205, 0.5086, 0.5142,  ..., 0.4814, 0.4960, 0.4837],
         [0.5205, 0.5086, 0.5142,  ..., 0.4814, 0.4960, 0.4837]],
        [[0.5188, 0.4918, 0.5162,  ..., 0.4758, 0.5066, 0.4918],
         [0.5106, 0.5293, 0.5068,  ..., 0.4712, 0.4953, 0.4880],
         [0.5128, 0.5159, 0.5099,  ..., 0.4724, 0.4946, 0.4959],
         ...,
         [0.5056, 0.5136, 0.5057,  ..., 0.4766, 0.4887, 0.4708],
         [0.5070, 0.5139, 0.5047,  ..., 0.4760, 0.4894, 0.4726],
         [0.5078, 0.5200, 0.5067,  ..., 0.4719, 0.4892, 0.4859]],
        [[0.5054, 0.4936, 0.5212,  ..., 0.4824, 0.5031, 0.4842],
         [0.5115, 0.5049, 0.5138,  ..., 0.4834, 0.4992, 0.4949],
         [0.5101, 0.4974, 0.5161,  ..., 0.4809, 0.4952, 0.4917],
         ...,
         [0.5156, 0.4970, 0.5114,  ..., 0.4766, 0.4983, 0.4891],
         [0.5156, 0.4970, 0.5114,  ..., 0.4766, 0.4983, 0.4891],
         [0.5156, 0.4970, 0.5114,  ..., 0.4766, 0.4983, 0.4891]]],
       device='cuda:0', grad_fn=<SigmoidBackward0>)
(Pdb) 0
(Pdb)
Training:    :   0%|          | 0/160 [01:04<?, ?it/s]
Traceback (most recent call last):
  File "D:\ETHz\Internship\adaptive-e-learning-for-educational-recommendation-system\DeepKnowledgeTracing-DKT-Pytorch\DKT\KnowledgeTracing\evaluation\run.py", line 41, in <module>
    model, optimizer, train_loss = eval.train(trainLoaders, model, optimizer_adgd, loss_func,device)
  File "..\evaluation\eval.py", line 90, in train
    model, optimizer, loss = train_epoch(model, trainLoaders[i], optimizer, lossFunc,device)
  File "..\evaluation\eval.py", line 48, in train_epoch
    optimizer.zero_grad()
  File "C:\Users\tyu06\anaconda3\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\tyu06\anaconda3\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\tyu06\anaconda3\lib\site-packages\torch\optim\adagrad.py", line 106, in step
    F.adagrad(params_with_grad,
  File "C:\Users\tyu06\anaconda3\lib\site-packages\torch\optim\_functional.py", line 54, in adagrad
    state_sum.addcmul_(grad, grad, value=1)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!